
from __future__ import annotations

import math
from typing import List, Literal, Optional

from PIL import Image


from safetensors.torch import load_file
import torch
from torch import Tensor
from torch import nn
import torch.nn.functional as F
from transformers.models.auto import CONFIG_MAPPING
from transformers.models.gemma import modeling_gemma
from torch.nn.attention.flex_attention import create_block_mask

from openpi import models_pytorch as _mpp  # å®˜æ–¹
from openpi.models import gemma as _gemma  # å®˜æ–¹
from openpi.models_pytorch import preprocessing_pytorch as _pp  # å®˜æ–¹
from openpi.models_pytorch.gemma_pytorch import PaliGemmaWithExpertModel  # å®˜æ–¹
from openpi.vlm_expert.dinov2_with_registers import Dinov2WithRegistersConfig
from openpi.vlm_expert.dinov2_with_registers import Dinov2WithRegistersModel
from openpi.vlm_expert.g2vlm import G2VLM
from openpi.vlm_expert.g2vlm import Dinov2WithRegistersConfig
from openpi.vlm_expert.g2vlm import Dinov2WithRegistersModel
from openpi.vlm_expert.g2vlm import G2VLMConfig
from openpi.vlm_expert.g2vlm import Qwen2VLConfig
from openpi.vlm_expert.g2vlm import Qwen2VLForCausalLM
from openpi.vlm_expert.g2vlm.qwen2vl import Qwen2VLForCausalLM
from openpi.vlm_expert.qwen2 import Qwen2Tokenizer
from openpi.vlm_expert.qwen2.configuration_qwen2 import Qwen2Config
from openpi.vlm_expert.qwen2.modeling_qwen2 import Qwen2ForCausalLM
from openpi.vlm_expert.qwen2vl.configuration_qwen2_vl import Qwen2VLVisionConfig
from openpi.vlm_expert.qwen2vl.modeling_qwen2_vl import Qwen2VisionTransformerPretrainedModel
from openpi.vlm_expert.qwen2vl.modeling_qwen2_vl import apply_multimodal_rotary_pos_emb
from openpi.vlm_expert.qwen2vl.modeling_qwen2_vl_vit import Qwen2VisionTransformerPretrainedModel

from ..data_vlm.data_utils import add_special_tokens, create_sparse_mask
from ..data_vlm.data_utils import pil_img2rgb
from ..data_vlm.transforms import ImageTransform
from ..data_vlm.transforms import InternVLImageTransform
from ..data_vlm.transforms import QwenVL2ImageTransform
from ..data_vlm.transforms_vggt import DinoImageNormalizeTransform
from ..data_vlm.transforms_vggt import DinoImageTransform

from ..data_vlm.data_utils import (
    create_sparse_mask, 
    get_flattened_position_ids_extrapolate, 
    get_flattened_position_ids_interpolate,
    get_rope_index_image_3D,
    get_rope_index_image_3D_dino,
    patchify, 
)

from openpi.vlm_expert.g2vlm.qwen2vl import Qwen2VLForCausalLM, Qwen2VLConfig, NaiveCache
import logging
import sys
from pathlib import Path
from typing import Literal

import os

# Copied from transformers.models.llama.modeling_llama.rotate_half
def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

def apply_mrope_to_expert(q, k, cos, sin):
    """
    é’ˆå¯¹ Qwen2-VL çš„ M-RoPE é€»è¾‘ï¼šå°† head_dim æ‹†åˆ†ä¸º T, H, W ä¸‰éƒ¨åˆ†åˆ†åˆ«æ—‹è½¬
    q, k: [Batch, Heads, Seq, Dim]
    cos, sin: [3, Batch, Seq, Dim] (ç”± rope_module ç”Ÿæˆ)
    """
    # 1. æŒ‰ç…§ Qwen2-VL å®˜æ–¹æ¯”ä¾‹æ‹†åˆ† head_dim (1/2, 1/4, 1/4)
    dim = cos.shape[-1]
    m_cos = torch.cat([
        cos[0, ..., :dim//2],          # Temporal
        cos[1, ..., dim//2:3*dim//4],    # Height
        cos[2, ..., 3*dim//4:]           # Width
    ], dim=-1)
    
    m_sin = torch.cat([
        sin[0, ..., :dim//2],
        sin[1, ..., dim//2:3*dim//4],
        sin[2, ..., 3*dim//4:]
    ], dim=-1)

    # 2. å¢åŠ  Heads ç»´åº¦ç”¨äºå¹¿æ’­: [B, 1, L, D]
    m_cos = m_cos.unsqueeze(1)
    m_sin = m_sin.unsqueeze(1)

    # 3. æ‰§è¡Œæ—‹è½¬ (FP32 è®¡ç®—ä»¥ä¿ç¨³)
    q_out = (q.float() * m_cos) + (rotate_half(q.float()) * m_sin)
    k_out = (k.float() * m_cos) + (rotate_half(k.float()) * m_sin)
    
    return q_out.to(q.dtype), k_out.to(k.dtype)


# Add 20250110
def apply_rotary_pos_emb_vision_3d(q, k, cos, sin):
    """
    é’ˆå¯¹ Qwen2-VL M-RoPE çš„ 3D æ—‹è½¬åº”ç”¨
    q, k: [Batch, Heads, Seq, Dim]
    cos, sin: [3, Batch, Seq, Dim] (ç”± rope_module ç”Ÿæˆ)
    """
    # 1. ç»´åº¦å¯¹é½ï¼šå°† cos/sin æ’å…¥ Heads ç»´åº¦ä»¥ä¾¿å¹¿æ’­ [3, B, 1, L, D]
    cos = cos.unsqueeze(2)
    sin = sin.unsqueeze(2)
    
    # 2. æ ¸å¿ƒé€»è¾‘ï¼šQwen2-VL å°† head_dim æ‹†åˆ†ä¸º T, H, W ä¸‰éƒ¨åˆ†
    # é€šå¸¸æ¯”ä¾‹ä¸º: T(1/2), H(1/4), W(1/4)
    dim = q.shape[-1]
    
    # æ„é€ æ··åˆæ—‹è½¬çŸ©é˜µ
    # è¿™ç§æ–¹å¼ä¿è¯äº† q çš„ä¸åŒé€šé“åˆ†åˆ«å¸æ”¶äº†ä¸åŒè½´çš„ä½ç½®ä¿¡æ¯
    m_cos = torch.cat([
        cos[0, ..., :dim//2],          # æ—¶é—´åˆ†é‡æ—‹è½¬å‰ä¸€åŠç»´åº¦
        cos[1, ..., dim//2:3*dim//4],    # é«˜åº¦åˆ†é‡æ—‹è½¬ä¸­é—´ 1/4
        cos[2, ..., 3*dim//4:]           # å®½åº¦åˆ†é‡æ—‹è½¬æœ€å 1/4
    ], dim=-1)
    
    m_sin = torch.cat([
        sin[0, ..., :dim//2],
        sin[1, ..., dim//2:3*dim//4],
        sin[2, ..., 3*dim//4:]
    ], dim=-1)

    # 3. æ‰§è¡Œæ—‹è½¬è®¡ç®—
    def rotate_half(x):
        x1 = x[..., : x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2 :]
        return torch.cat((-x2, x1), dim=-1)

    # æå‡ç²¾åº¦è®¡ç®—é˜²æ­¢ NaN
    orig_dtype = q.dtype
    q, k = q.float(), k.float()
    
    q_embed = (q * m_cos) + (rotate_half(q) * m_sin)
    k_embed = (k * m_cos) + (rotate_half(k) * m_sin)
    
    return q_embed.to(orig_dtype), k_embed.to(orig_dtype)

def get_rope_index_for_hidden(attention_mask: torch.Tensor):
    """
    Returns position_ids of shape (batch, seq_len) compatible with rotary_emb
    """
    if attention_mask is None:
        raise ValueError("attention_mask must be provided")

    # cumsum ç”Ÿæˆ position_ids
    position_ids = attention_mask.long().cumsum(-1) - 1
    position_ids.masked_fill_(attention_mask == 0, 0)
    return position_ids  # (batch, seq_len)


def build_transform(pixel=224):
    image_transform = QwenVL2ImageTransform(pixel, pixel, 14)

    return image_transform

def load_model_and_tokenizer(model_path):
    llm_config = Qwen2VLConfig.from_json_file(os.path.join(model_path, "text_config.json"))

    llm_config.qk_norm = True
    llm_config.tie_word_embeddings = False
    llm_config.layer_module = 'Qwen2VLMoTDecoderLayer'  

    vit_config = Qwen2VLVisionConfig.from_json_file(os.path.join(model_path, "vit_config.json"))
    vit_config.patch_size =14

    dino_config = Dinov2WithRegistersConfig.from_json_file(os.path.join(model_path, "dino_config.json"))

    config = G2VLMConfig(
        visual_und=True,
        visual_recon=True, # Dino use
        llm_config=llm_config, 
        vit_config=vit_config,
        dino_config=dino_config,
        vit_max_num_patch_per_side=36,
    )
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    language_model = Qwen2VLForCausalLM(llm_config).to(device)
    vit_model      = Qwen2VisionTransformerPretrainedModel(vit_config).to(device)
    dino_model = Dinov2WithRegistersModel(dino_config).to(device)

    model = G2VLM(language_model, vit_model, dino_model, config)

    tokenizer = Qwen2Tokenizer.from_pretrained(model_path)
    tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)

    vit_image_transform = QwenVL2ImageTransform(768, 768, 14)
    dino_transform = DinoImageNormalizeTransform(target_size=518)

    model_state_dict_path = os.path.join(model_path, "model.safetensors")
    model_state_dict = load_file(model_state_dict_path, device="cpu")
    msg = model.load_state_dict(model_state_dict, strict=False)
    print(msg)
    del model_state_dict
    model = model.cuda().eval()

    return model, tokenizer, new_token_ids , vit_image_transform, dino_transform, llm_config

# ---------- 1. ä¸‰ä¸“å®¶ MoT ----------
class G2VLMWithActorExpertModel(nn.Module):
    """
    å®˜æ–¹ PaliGemmaWithExpertModel çš„â€œä¸‰ä¸“å®¶â€ç‰ˆï¼š
    - prefix:  image+text  â†’  Semantic  Expert (PaliGemma, å†»ç»“)
    - prefix:  dinoâ†’3D     â†’  Geometric Expert (G2VLM, å†»ç»“)
    - suffix:  state+action â†’ Action    Expert (Gemma-300M, å¯è®­)
    å…±äº« Self-Attentionï¼ŒFFN æŒ‰ token ç±»å‹è·¯ç”±ã€‚
    """

    """G2VLM model with action expert for PI0, replacing PaliGemmaWithExpertModel."""

    def __init__(
        self,
        g2_vlm_path,
        action_expert_config,
        use_adarms=None,
        precision: Literal["bfloat16", "float32"] = "bfloat16",
        image_size: int = 224,
    ):
        if use_adarms is None:
            use_adarms = [False, False]
        super().__init__()

        # If G2VLM model is provided, use it directly
        g2_model, tokenizer, new_token_ids , vit_image_transform, dino_transform, llm_config= load_model_and_tokenizer(g2_vlm_path)
        device = g2_model.device
        self.g2vlm = g2_model.to(device = device)
        self.vit_image_transform = build_transform()# set 224
        # self.vit_image_transform = self.vit_image_transform.to(device = device)
        self.visiontower = g2_model.vit_model.to(device = device)
        self.dino_transform = dino_transform
        self.dinoTower = g2_model.dino_model.to(device = device)
        self.dinoProjector = g2_model.dino2llm
        self.llm_config = llm_config


        # Create action expert (Gemma model) similar to PaliGemmaWithExpertModel
        from transformers.models.auto import CONFIG_MAPPING
        from transformers.models.gemma import GemmaForCausalLM

        action_expert_config_hf = CONFIG_MAPPING["gemma"](
            head_dim=128,
            hidden_size=llm_config.hidden_size,
            intermediate_size=action_expert_config.mlp_dim,
            num_attention_heads=llm_config.num_attention_heads, # need same as G@VLM
            num_hidden_layers= 28, # need = to g2vlm language
            num_key_value_heads=llm_config.num_key_value_heads,
            vocab_size=self.g2vlm.language_model.vocab_size,  # Match PaliGemma vocab size
            hidden_activation="gelu_pytorch_tanh",
            torch_dtype="float32",
            use_adarms=use_adarms[1],
            adarms_cond_dim=action_expert_config.width if use_adarms[1] else None,
        )

        self.action_expert = GemmaForCausalLM(config=action_expert_config_hf).to(device = device)
        self.action_expert.model.embed_tokens = None  # We'll use shared embeddings

        self.action_gate = nn.Parameter(torch.ones(28))
        
        # å­˜å‚¨å½“å‰ batch çš„ grid ä¿¡æ¯ï¼Œç”¨äºæ„å»º position_ids
        self.current_vit_grid = []
        self.current_dino_grid = []

        # action_expert_config_hf = Qwen2Config(
        #     hidden_size=llm_config.hidden_size,
        #     intermediate_size=action_expert_config.mlp_dim,
        #     num_hidden_layers=action_expert_config.depth,
        #     num_attention_heads = llm_config.num_attention_heads, # need same as G@VLM
        #     num_key_value_heads=llm_config.num_key_value_heads,
        #     vocab_size=self.g2vlm.language_model.vocab_size,
        #     torch_dtype=torch.float32,
        #     attention_dropout=0.0,
        # )

        # self.action_expert = Qwen2ForCausalLM(
        #     config=action_expert_config_hf
        # ).to(device)
        # self.action_expert.model.embed_tokens = None

        self.to_bfloat16_for_selected_params(precision)

    def to_bfloat16_for_selected_params(self, precision: Literal["bfloat16", "float32"] = "bfloat16"):
        if precision == "bfloat16":
            self.to(dtype=torch.bfloat16)
        elif precision == "float32":
            self.to(dtype=torch.float32)
            return
        else:
            raise ValueError(f"Invalid precision: {precision}")

        params_to_keep_float32 = [
            "input_layernorm",
            "post_attention_layernorm",
            "model.norm",
        ]

        for name, param in self.named_parameters():
            if any(selector in name for selector in params_to_keep_float32):
                param.data = param.data.to(dtype=torch.float32)

    def _ensure_transforms(self):
        """Lazily create image transforms for VIT and DINO, following G2VLM's own pipeline."""
        if getattr(self, "_vit_transform", None) is None:
            if "QwenVL2ImageTransform" in globals() and QwenVL2ImageTransform is not None:
                # Use the same settings as g2vlm_utils.load_model_and_tokenizer
                self._vit_transform = QwenVL2ImageTransform(768, 768, 14)
            else:
                logging.warning("QwenVL2ImageTransform is not available; VIT embeddings will be disabled.")
                self._vit_transform = None

        if getattr(self, "_dino_transform", None) is None:
            if "DinoImageTransform" in globals() and DinoImageTransform is not None:
                self._dino_transform = DinoImageTransform(target_size=518)
            else:
                logging.warning("DinoImageTransform is not available; DINO embeddings will be disabled.")
                self._dino_transform = None


    def embed_image(self, image: torch.Tensor):
        """
        image: Tensor[B, C, H, W]  (raw RGB, 0~1 or 0~255ï¼Œå–å†³äº transform)
        return:
            {
                "semantic":  Tensor[B, N_vit, D],
                "geometric": Tensor[B, N_dino, D],
                "vit_grid":  Tensor[1, 3],  # [T, H, W]
                "dino_grid": Tensor[1, 3],  # [T, H, W]
            }

        Add 20250110: è¿”å› grid grid_thw ä¿¡æ¯
        """

        # --- ğŸš€ æ ¸å¿ƒè¯Šæ–­ï¼šæ£€æŸ¥è¾“å…¥åƒç´  ---
        print("-" * 30)
        print(f"DEBUG [Image Raw]: dtype: {image.dtype}")
        print(f"DEBUG [Image Raw]: shape: {image.shape}")
        print(f"DEBUG [Image Raw]: min: {image.min().item():.4f}")
        print(f"DEBUG [Image Raw]: max: {image.max().item():.4f}")
        print(f"DEBUG [Image Raw]: mean: {image.mean().item():.4f}")
        print("-" * 30)

        # --- ğŸš€ ä¿å‘½é” 1ï¼šé˜²æ­¢å…¨é»‘å›¾ç‰‡å¯¼è‡´ NaN ---
        # å¦‚æœå›¾åƒæ‰€æœ‰å€¼éƒ½ä¸€æ ·ï¼ˆæ–¹å·®ä¸º0ï¼‰ï¼Œç»™å®ƒåŠ ä¸€ç‚¹ç‚¹æå…¶å¾®å°çš„å™ªå£°
        # æ— æ¡ä»¶åŠ ä¸€ä¸ªæå°çš„æ‰°åŠ¨ (1e-6 å‡ ä¹ä¸å½±å“è®­ç»ƒï¼Œä½†èƒ½é˜²æ­¢å…¨å¹³å›¾åƒ)
        # æˆ–è€…ç›´æ¥æŠŠ allclose ç§»åˆ° Gradient Checkpoint ä¹‹å¤–
        image = image + torch.randn_like(image) * 1e-6

        device = image.device
        B, C, H, W = image.shape

        # å¦‚æœåªæœ‰ä¸€å¼ å›¾
        if image.dim() == 3:                      # [3, H, W]
            image = image.unsqueeze(0)            # [1, 3, H, W]

        # ---------- 1. è¯­ä¹‰åˆ†æ”¯ (Qwen2-VL ViT) ----------


        vit_pixel_values, image_grid_thw = self.vit_image_transform(image)
        print(f"DEBUG: vit_pixel_values max: {vit_pixel_values.abs().max()}")
        
        device = next(self.visiontower.parameters()).device
        dtype = next(self.visiontower.parameters()).dtype

        vit_pixel_values = vit_pixel_values.to(device=device, dtype=dtype)
        image_grid_thw = image_grid_thw.to(device=device)
        vit_grid_thw = image_grid_thw.to(device=device)

        # 1.3 ä¸€æ¬¡ forward æ‹¿ç‰¹å¾
        vit_feats = self.visiontower(vit_pixel_values, grid_thw=image_grid_thw)  # [B, N_vit, D]



        # ---------- 2. å‡ ä½•åˆ†æ”¯ (DINO) ----------
        dino_images = self.dino_transform(image)          # -> [B, C, H'', W'']
        print(f"DEBUG [DINO]: input images max: {dino_images.abs().max().item():.4f}")

        B, C, H, W = dino_images.shape
        patch_size = self.dinoTower.config.patch_size  # ä¾‹å¦‚ 16


        patch_size = self.dinoTower.config.patch_size
        dino_h_tokens = dino_images.shape[2] // patch_size
        dino_w_tokens = dino_images.shape[3] // patch_size
        # æ„é€ ç¬¦åˆ Qwen2-VL æ ¼å¼çš„ grid_thw: [T, H, W]
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾æ˜¯å•å¼ å›¾ï¼Œå¦‚æœæ˜¯è§†é¢‘éœ€è¦æ ¹æ® B è°ƒæ•´ï¼Œä½†åœ¨ VLA ä¸­é€šå¸¸ B æ”¾åœ¨å¤–é¢
        dino_grid_thw = torch.tensor(
            [[1, dino_h_tokens, dino_w_tokens]], 
            device=dino_images.device, 
            dtype=torch.int32
        )

        num_tokens_per_image = (H // patch_size) * (W // patch_size)  # æ¯å¼ å›¾çš„ token æ•°
        cu_seqlens = torch.arange(0, B * num_tokens_per_image + 1, num_tokens_per_image, 
                                  device=dino_images.device,
                                  dtype=torch.int32 
                                  )
        max_seqlen = num_tokens_per_image


        dino_out = self.dinoTower(dino_images, cu_seqlens, max_seqlen)
        if torch.isnan(dino_out).any():
            print("âŒ NaN detected inside dinoTower!")
            # å°è¯•å¼ºåˆ¶ä¿®å¤ (ä»…ç”¨äºè°ƒè¯•)
            dino_out = torch.nan_to_num(dino_out, 0.0)
        # dino_feats = dino_out.last_hidden_state

        
        # dino_feats = self.dinoTower(pixel_values=dino_images)        # [B, N_dino, dino_dim]
        geometric_tokens = self.dinoProjector(dino_out)            # [B, N_dino, D]
        if torch.isnan(geometric_tokens).any():
            print("âŒ NaN detected after dinoProjector!")

        # ---------- 3. å­˜å‚¨ grid ä¿¡æ¯ï¼ˆç”¨äºæ„å»º position_idsï¼‰----------
        # æ³¨æ„ï¼šè¿™é‡Œåªå­˜å‚¨å•å¼ å›¾çš„ gridï¼Œå¦‚æœæ˜¯ batchï¼Œéœ€è¦åœ¨è°ƒç”¨å¤„ç´¯ç§¯
        # åœ¨ omni_vla.py çš„ embed_prefix ä¸­ä¼šç´¯ç§¯è¿™äº›ä¿¡æ¯
        
        # ---------- 4. è¿”å› ----------
        return {
            "semantic": vit_feats,      # [B, N_vit, D]
            "geometric": geometric_tokens,  # [B, N_dino, D]
            "vit_grid": vit_grid_thw[0],   # [1, 3] -> ç”¨äº build_3d_position_ids
            "dino_grid": dino_grid_thw[0], # [1, 3] -> ç”¨äº build_3d_position_ids
        }


    def embed_language_tokens(self, tokens: torch.Tensor):
        """Embed language tokens using G2VLM's language model."""
        return self.g2vlm.language_model.model.embed_tokens(tokens)
    
    def build_prefix(
        self,
        image: torch.Tensor,
        text_tokens: torch.Tensor,
    ):
        """
        return:
            prefix_embeds:     [B, N, D]
            prefix_token_type: [B, N]

            token_type == 0 â†’ semantic expert
            token_type == 1 â†’ geometric expert
            token_type == 2 â†’ language expert

        """

        image_embeds = self.embed_image(image)

        semantic_tokens = image_embeds["semantic"]    # [B, Ns, D]
        geometric_tokens = image_embeds["geometric"]  # [B, Ng, D]
        text_embeds = self.embed_language_tokens(text_tokens)  # [B, T, D]

        # --- ğŸš€ å…³é”®è¯Šæ–­ï¼šçœ‹çœ‹æ˜¯è°å¸¦æ¯’ ---
        print(f"DEBUG: semantic_tokens max: {semantic_tokens.abs().max()}")
        print(f"DEBUG: geometric_tokens max: {geometric_tokens.abs().max()}")
        print(f"DEBUG: text_embeds max: {text_embeds.abs().max()}")

        prefix_embeds = torch.cat(
            [semantic_tokens, geometric_tokens, text_embeds],
            dim=1,
        )

        token_type_ids = self.build_prefix_token_type_ids(
            semantic_tokens,
            geometric_tokens,
            text_embeds,
        )

        return prefix_embeds, token_type_ids

    @staticmethod
    def _gated_residual(x, y, gate):
        """
        Applies gated residual connection with optional gate parameter. 
        
        Args:
            x: Input tensor (residual)
            y: Output tensor to be added
            gate: Optional gate tensor to modulate the addition
            
        Returns:
            x + y if gate is None, otherwise x + y * gate
        """
        if x is None and y is None:
            return None
        if x is None or y is None:
            return x if x is not None else y
        if gate is None:
            return x + y
        return x + y * gate
    
    
    def forward(
        self,
        attention_mask: torch.Tensor | None = None,
        position_ids: torch.LongTensor | None = None,
        past_key_values=None,
        inputs_embeds: list[torch.FloatTensor] | None = None,
        use_cache: bool | None = None,
        adarms_cond: list[torch.Tensor] | None = None,
    ):
        """
        inputs_embeds:
            [0] prefix embeds  (semantic + geometric + text)
            [1] suffix embeds  (action tokens)
        """
        if adarms_cond is None:
            adarms_cond = [None, None]

        # --------------------------------------------------
        # Case 1: only prefix (encode / prefill)
        # --------------------------------------------------
        if inputs_embeds[1] is None:
            prefix_output = self.g2vlm.language_model.forward(
                inputs_embeds=inputs_embeds[0],
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                use_cache=use_cache,
                adarms_cond=adarms_cond[0],
            )
            prefix_past_key_values = prefix_output.past_key_values
            prefix_output = prefix_output.last_hidden_state
            suffix_output = None

        # --------------------------------------------------
        # Case 2: only suffix (decode action)
        # --------------------------------------------------
        elif inputs_embeds[0] is None:
            suffix_output = self.action_expert.model.forward(
                inputs_embeds=inputs_embeds[1],
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                use_cache=use_cache,
                adarms_cond=adarms_cond[1],
            )
            suffix_output = suffix_output.last_hidden_state
            prefix_output = None
            prefix_past_key_values = None

        # --------------------------------------------------
        # Case 3: prefix + suffix joint attention (PI-0 core)
        # --------------------------------------------------
        else:
            # ğŸ”‘ å’ŒåŸ PI-0 å®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯æ¢äº† prefix model
            models = [
                self.g2vlm.language_model,   # prefix expert (semantic + geometric + text)
                self.action_expert.model,     # suffix expert (action)
            ]

            num_layers = models[0].config.num_hidden_layers

            # debug
            for i, x in enumerate(inputs_embeds):
                if x is not None:
                    print(f"Expert {i} input max: {x.abs().max()}")
            
            # ç¡®ä¿ grid åˆ—è¡¨å·²åˆå§‹åŒ–ï¼ˆåœ¨ embed_prefix ä¸­ä¼šè¢«å¡«å……ï¼‰
            if not hasattr(self, 'current_vit_grid'):
                self.current_vit_grid = []
            if not hasattr(self, 'current_dino_grid'):
                self.current_dino_grid = []



            # å¦‚æœä½ æ²¡æœ‰çœŸå®çš„ full_input_idsï¼Œè‡³å°‘éœ€è¦æ ¹æ®é•¿åº¦æ„é€ ä¸€ä¸ª LongTensor
            # æ³¨æ„ï¼šå¿…é¡»æ˜¯ Long ç±»å‹
            batch_size = inputs_embeds[0].shape[0]
            prefix_len = inputs_embeds[0].shape[1]
            suffix_len = inputs_embeds[1].shape[1]
            total_len = prefix_len + suffix_len

            # å¦‚æœ position_ids ä¸º Noneï¼Œä½¿ç”¨å­˜å‚¨çš„ grid ä¿¡æ¯æ„å»º
            if position_ids is None:
                # ä½¿ç”¨å’Œ omni_vla.py ä¸­ç›¸åŒçš„æ–¹æ³•æ„å»º 3D position_ids
                from ..data_vlm.data_utils import get_rope_index_image_3D
                
                device = inputs_embeds[0].device
                b = batch_size
                curr_pos_val = 0
                
                # 1. æ„å»º ViT (è¯­ä¹‰) ä½ç½®ç¼–ç 
                all_vit_pos = []
                if hasattr(self, 'current_vit_grid') and len(self.current_vit_grid) > 0:
                    for grid in self.current_vit_grid:
                        pos_3d, delta = get_rope_index_image_3D(
                            grid.flatten()[:3] if grid.dim() > 0 else grid[:3], 
                            curr_position_id=curr_pos_val
                        )
                        all_vit_pos.append(pos_3d.unsqueeze(1).repeat(1, b, 1))
                        curr_pos_val += int(delta) + 1
                
                # 2. æ„å»º DINO (å‡ ä½•) ä½ç½®ç¼–ç 
                all_dino_pos = []
                if hasattr(self, 'current_dino_grid') and len(self.current_dino_grid) > 0:
                    for grid in self.current_dino_grid:
                        pos_3d, delta = get_rope_index_image_3D(
                            grid.flatten()[:3] if grid.dim() > 0 else grid[:3], 
                            curr_position_id=curr_pos_val
                        )
                        all_dino_pos.append(pos_3d.unsqueeze(1).repeat(1, b, 1))
                        curr_pos_val += int(delta) + 1
                
                # 3. è®¡ç®—æ–‡æœ¬å’ŒåŠ¨ä½œçš„é•¿åº¦
                current_vision_len = sum([p.shape[-1] for p in all_vit_pos]) + sum([p.shape[-1] for p in all_dino_pos])
                actual_prefix_len = prefix_len
                text_len = actual_prefix_len - current_vision_len
                
                # 4. æ„å»ºæ–‡æœ¬å’ŒåŠ¨ä½œçš„ä½ç½®ç¼–ç ï¼ˆçº¿æ€§ T è½´ï¼‰
                total_incremental_len = text_len + suffix_len
                incremental_ids = torch.arange(curr_pos_val, curr_pos_val + total_incremental_len, device=device)
                text_act_pos = incremental_ids.unsqueeze(0).unsqueeze(0).repeat(3, b, 1)
                
                # 5. æ‹¼æ¥æ‰€æœ‰ä½ç½®ç¼–ç 
                all_pos = all_vit_pos + all_dino_pos
                if all_pos:
                    full_pos = torch.cat(all_pos + [text_act_pos], dim=-1)
                else:
                    # å¦‚æœæ²¡æœ‰è§†è§‰ä¿¡æ¯ï¼Œåªä½¿ç”¨æ–‡æœ¬å’ŒåŠ¨ä½œ
                    full_pos = text_act_pos
                
                position_ids = full_pos.to(device)
                
                # éªŒè¯é•¿åº¦
                expected_len = actual_prefix_len + suffix_len
                if position_ids.shape[-1] != expected_len:
                    logging.warning(
                        f"Position IDs length mismatch: got {position_ids.shape[-1]}, expected {expected_len}. "
                        f"Using fallback linear position encoding."
                    )
                    # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•çš„çº¿æ€§ä½ç½®ç¼–ç 
                    position_ids = torch.arange(expected_len, device=device).unsqueeze(0).unsqueeze(0).repeat(3, b, 1)

            # ç¡®ä¿ position_ids æ˜¯ 3 ç»´çš„: [3, B, L]
            if position_ids.dim() == 2:
                position_ids = position_ids.unsqueeze(0).repeat(3, 1, 1)

            # gradient checkpointingï¼ˆåŸæ ·ä¿ç•™ï¼‰
            use_gradient_checkpointing = (
                hasattr(self.action_expert.model, "gradient_checkpointing")
                and self.action_expert.model.gradient_checkpointing
                and self.training
            )

            if self.training and hasattr(self.action_expert.model, "gradient_checkpointing"):
                if not self.action_expert.model.gradient_checkpointing:
                    self.action_expert.model.gradient_checkpointing = True
                use_gradient_checkpointing = True

            def compute_layer_complete(
                layer_idx,
                inputs_embeds,
                attention_mask,
                position_ids,# è¿™é‡Œçš„ position_ids å¿…é¡»æ˜¯ [3, B, L] çš„ LongTensor
                adarms_cond,
            ):
                query_states = []
                key_states = []
                value_states = []
                gates = []

                for i, hidden_states in enumerate(inputs_embeds):

                    layer = models[i].base_model.layers[layer_idx]
                    hidden_states = layer.input_layernorm(hidden_states)  # ä¸ä¼  cond
                    # åˆ›å»ºå…¨ 1 gateï¼Œå ä½
                    gate = torch.full_like(hidden_states, 0.001)
                    gate = gate.to(hidden_states.dtype)
                        
                    device = layer.self_attn.q_proj.weight.device
                    dtype = layer.self_attn.q_proj.weight.dtype

                    hidden_states = hidden_states.to(device=device, dtype=dtype)
                    
                    gates.append(gate)

                    input_shape = hidden_states.shape[:-1]
                    hidden_shape = (*input_shape, -1, layer.self_attn.head_dim)

                    print(f"LayerNorm out max: {hidden_states.abs().max()}")

                    q = layer.self_attn.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
                    k = layer.self_attn.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
                    v = layer.self_attn.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

                    print(f"Q proj max: {q.abs().max()}")

                    query_states.append(q)
                    key_states.append(k)
                    value_states.append(v)

                # ğŸ”‘ concat å‰ç¡®è®¤ hidden_size å¯¹é½
                for i, x in enumerate(query_states):
                    assert x.shape[-1] == layer.self_attn.head_dim, f"Expert {i} Q shape mismatch"

                # --- æ‹¼æ¥æ‰€æœ‰ä¸“å®¶çš„ Token ---
                # query_states æ‹¼æ¥åçš„å½¢çŠ¶: [B, num_heads, total_seq_len, head_dim]
                # concat attention
                query_states = torch.cat(query_states, dim=2)
                key_states = torch.cat(key_states, dim=2)
                value_states = torch.cat(value_states, dim=2)

                print("Fixed Query Shape:", query_states.shape)
                print("Fixed key_states Shape:", key_states.shape)
                print("Fixed value_states Shape:", value_states.shape)


                # 1. è·å– 3D æ—‹è½¬é¢‘ç‡
                rope_module = models[0].base_model.layers[0].self_attn.rotary_emb

                prefix_len = inputs_embeds[0].shape[1]
                suffix_len = inputs_embeds[1].shape[1]
                total_len = prefix_len + suffix_len

                if position_ids.dim() == 2:
                    # æ‰©å±•ä¸º [3, batch_size, seq_len]
                    # Qwen2-VL æœŸæœ›ç¬¬ 0 ç»´æ˜¯ [T_index, H_index, W_index]
                    position_ids = position_ids.unsqueeze(0).repeat(3, 1, 1)

                # 2. è°ƒç”¨ rotary_emb çš„ forward
                # åœ¨ Qwen2-VL ä¸­ï¼Œrotary_emb(value_states, position_ids) ä¼šï¼š
                # a) æ ¹æ® position_ids (3, B, L) æå– T, H, W çš„ç´¢å¼•
                # b) é’ˆå¯¹ head_dim çš„ä¸åŒéƒ¨åˆ†è®¡ç®—å¯¹åº”çš„æ—‹è½¬é¢‘ç‡
                # c) è¿”å›ç¬¦åˆ M-RoPE è§„åˆ™çš„ cos å’Œ sin
                with torch.no_grad():
                    # cos, sin å½¢çŠ¶é€šå¸¸ä¸º [3, B, L, head_dim // (æŸå› å­)] 
                    # æˆ–è€…åœ¨æœ€æ–°ç‰ˆ HF ä¸­ç›´æ¥è¿”å›æ‹¼æ¥å¥½çš„å˜æ¢å¼ é‡
                    cos, sin = rope_module(value_states, position_ids)
                    print(f"Cos max: {cos.max()}, Sin max: {sin.max()}") # ğŸ‘ˆ æ£€æŸ¥è¿™é‡Œ


                # # 1. Handle M-RoPE 5D output (if it returns the 3-axis components)æ¨¡å‹ä¼šä¸¢å¤±é«˜åº¦å’Œå®½åº¦çš„ç©ºé—´åæ ‡ 
                # if cos.dim() == 5:
                #     # Most apply_rotary_pos_emb functions expect 4D.
                #     # Usually, we take index 0 or use a specific M-RoPE helper.
                #     cos = cos[0]
                #     sin = sin[0]

                # # 2. FORCE the head dimension to be 1 for broadcasting
                # # If shape is [Batch, 2, Seq, Dim], we want [Batch, 1, Seq, Dim]
                # if cos.shape[1] != 1:
                #     # We take only the first slice because RoPE is identical across heads
                #     cos = cos[:, :1, :, :]
                #     sin = sin[:, :1, :, :]

                print(f"Broadcast-ready Cos shape: {cos.shape}")

                # 2. åº”ç”¨ 3D M-RoPE
                query_states, key_states = apply_rotary_pos_emb_vision_3d(
                    query_states,
                    key_states,
                    cos,
                    sin
                )
                # q_embed, _ = apply_rotary_pos_emb(query_states, query_states, cos, sin)
                # _, k_embed = apply_rotary_pos_emb(key_states, key_states, cos, sin)
                # query_states = q_embed
                # key_states = k_embed

                print(f"Q max: {query_states.max()}, K max: {key_states.max()}")

                # query_states, key_states = modeling_gemma.apply_rotary_pos_emb(
                #     query_states, key_states, cos, sin, unsqueeze_dim=1
                # )

                # å°è¯•è·å–ï¼Œå¦‚æœè·å–ä¸åˆ°åˆ™æ‰‹åŠ¨è®¡ç®—
                attn_module = models[0].base_model.layers[layer_idx].self_attn
                if hasattr(attn_module, "scaling"):
                    scaling = attn_module.scaling
                else:
                    # head_dim é€šå¸¸æ˜¯ 128 æˆ– 64
                    scaling = attn_module.head_dim ** -0.5

                print(query_states.shape)
                print(key_states.shape)
                print(value_states.shape)
                print(attention_mask.shape)

                # if query_states.dim() == 5:
                #     # Qwen2-VL çš„ apply_mrope å¯èƒ½ä¼šä¿ç•™ 3D ç»´åº¦ã€‚
                #     # å®é™…ä¸Š M-RoPE å·²ç»å®Œæˆäº†æ—‹è½¬ï¼Œæˆ‘ä»¬åªéœ€è¦å–å…¶ä¸­ä¸€ä¸ªåˆ†é‡æˆ–è€…å¯¹é½ç»´åº¦ã€‚
                #     # åœ¨æ ‡å‡†å®ç°ä¸­ï¼Œæ—‹è½¬æ˜¯åŸä½çš„ï¼Œæˆ‘ä»¬é€šè¿‡ view æŠŠå®ƒå‹å› 4 ç»´ã€‚
                #     # æ³¨æ„ï¼šè¿™é‡Œå– query_states[0] æ˜¯ä¸è¡Œçš„ï¼Œå› ä¸ºä¸‰ä¸ªåˆ†é‡åˆ†åˆ«æ—‹è½¬äº†ä¸åŒçš„ head éƒ¨åˆ†ã€‚
                #     # æ­£ç¡®åšæ³•æ˜¯ view æˆ 4 ç»´ï¼Œå› ä¸º num_heads å·²ç»åŒ…å«äº†æ‰€æœ‰çš„ä¿¡æ¯ã€‚
                    
                #     b_size = value_states.shape[0] # çœŸå®çš„ Batch Size (1)
                    
                #     # æ£€æŸ¥ query_states çš„æ€»ç»´åº¦æ˜¯å¦åŒ¹é…
                #     # å¦‚æœæ˜¯ [3, B, H, L, D]ï¼Œé€šå¸¸ Qwen ä¼šåœ¨å†…éƒ¨æŠŠ H åˆ‡åˆ†ï¼Œ
                #     # ä½†å¦‚æœ apply_mrope è¿”å›çš„æ˜¯ 5 ç»´ï¼Œè¯´æ˜å®ƒæ²¡æœ‰è‡ªåŠ¨ squeezeã€‚
                #     query_states = query_states[0]
                #     key_states = key_states[0]
                #     # query_states = query_states.view(batch_size, num_heads, seq_len, head_dim)
                #     # key_states = key_states.view(batch_size, num_heads, seq_len, head_dim)
                    

                #     # å†æ¬¡æ‰“å°ç¡®è®¤ï¼Œåº”è¯¥æ˜¯ [1, 12, 1059, 128] è¿™ç§ 4 ç»´æ ¼å¼
                #     print("Fixed Query Shape:", query_states.shape)
                #     print("Fixed key Shape:", key_states.shape)

                # # ç¡®ä¿æ˜¯ 4D ä¸”ç»´åº¦å¯¹é½
                # batch_size = value_states.shape[0]
                # seq_len = value_states.shape[2]

                # # å¼ºåˆ¶æŒ‡å®šç»´åº¦ï¼Œé˜²æ­¢ view è‡ªåŠ¨ç›¸ä¹˜
                # query_states = query_states.reshape(batch_size, 12, seq_len, 128)
                # key_states = key_states.reshape(batch_size, 2, seq_len, 128)

                # æ‰“å°ä¸€ä¸‹ç¡®è®¤ï¼šåº”è¯¥æ˜¯ [1, 12, 1059, 128] å’Œ [1, 2, 1059, 128]
                print(f"Final Q: {query_states.shape}, K: {key_states.shape}")

                att_output, _ = modeling_gemma.eager_attention_forward(
                    models[0].base_model.layers[layer_idx].self_attn,
                    query_states,
                    key_states,
                    value_states,
                    attention_mask,
                    scaling,
                )

                head_dim = models[0].base_model.layers[layer_idx].self_attn.head_dim
                num_heads = models[0].base_model.layers[layer_idx].self_attn.num_heads
                att_output = att_output.reshape(att_output.shape[0], -1, num_heads * head_dim)

                outputs = []
                start = 0
                for i, hidden_states in enumerate(inputs_embeds):
                    layer = models[i].base_model.layers[layer_idx]
                    expert_dtype = layer.mlp.gate_proj.weight.dtype

                    end = start + hidden_states.shape[1]

                    out = layer.self_attn.o_proj(att_output[:, start:end])
                    out = self._gated_residual(hidden_states, out, gates[i])
                    

                    residual = out.clone()
                    # out, gate = layer.post_attention_layernorm(out, cond=adarms_cond[i])
                    out = layer.post_attention_layernorm(out)
                    gate = torch.ones_like(hidden_states)
                    out = out.to(expert_dtype)
                    out = layer.mlp(out)
                    out = self._gated_residual(residual, out, gate)

                    outputs.append(out)
                    start = end

                return outputs

            for layer_idx in range(num_layers):
                if use_gradient_checkpointing:
                    inputs_embeds = torch.utils.checkpoint.checkpoint(
                        compute_layer_complete,
                        layer_idx,
                        inputs_embeds,
                        attention_mask,
                        position_ids,
                        adarms_cond,
                        use_reentrant=False,
                    )
                else:
                    inputs_embeds = compute_layer_complete(
                        layer_idx,
                        inputs_embeds,
                        attention_mask,
                        position_ids,
                        adarms_cond,
                    )

            # final norm
            outputs = []
            for i, hidden_states in enumerate(inputs_embeds):
                out = models[i].base_model.norm(hidden_states)
                outputs.append(out)

            prefix_output, suffix_output = outputs
            prefix_past_key_values = None

        return [prefix_output, suffix_output], prefix_past_key_values
